{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate mock data"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "45k7s_w9O8El"
      },
      "id": "45k7s_w9O8El"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "# Part 1: Generate Mock Data\n",
        "data_dir = \"data\"\n",
        "dict_dir=\"mock_classes.json\"\n",
        "# Generating mock image names and a JSON file\n",
        "num_images = 1000\n",
        "class_labels = [\"cat\", \"dog\", \"bird\", \"fish\"]\n",
        "mock_image = np.array(np.random.randint(0, 256, [5, 5, 3]), dtype=np.uint8)\n",
        "mock_json = {\n",
        "    1: [\"18274\", \"cat\"],\n",
        "    2: [\"15938\", \"dog\"],\n",
        "    3: [\"70382\", \"bird\"],\n",
        "    4: [\"28673\", \"fish\"],\n",
        "}\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "for i in range(500):\n",
        "    for class_idx in mock_json:\n",
        "        cv2.imwrite(os.path.join(data_dir, f\"{mock_json[class_idx][0]}_{i:04d}.JPEG\"), mock_image)\n",
        "json.dump(mock_json, open(\"mock_classes.json\", \"w\"))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZjVu03CCO8Em"
      },
      "id": "ZjVu03CCO8Em"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1\n",
        "Read image xxxxx_xxxx.jpeg (imageID_xxxx.jpeg). Return its class index and class label.\n",
        "Input:\n",
        ".json dictionary. key: class index, value: [imageID, class label]\n",
        "image folder\n",
        "Output:\n",
        "[class index, class label]\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "c3949326fda67fe"
      },
      "id": "c3949326fda67fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "check_dict = json.load(open(dict_dir, \"r\"))\n",
        "imgid_class = {}\n",
        "for class_idx in check_dict:\n",
        "    imgid_class[check_dict[class_idx][0]] = [int(class_idx), check_dict[class_idx][1]]\n",
        "\n",
        "def get_idx_lbl(img_name):\n",
        "    segments = img_name.split(\"_\")\n",
        "    imgid = segments[0]\n",
        "    return imgid_class[imgid]\n",
        "\n",
        "# for item in os.listdir(data_dir):\n",
        "#     print(get_idx_lbl(item))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prediction\n",
        "Randomly sample 500 images in the directory.\n",
        "\n",
        "Create a 4-d tensorflow tensor X.\n",
        "\n",
        "get the corresponding labels with the method provided in step 1.\n",
        "\n",
        "Shuffle the images. Predict with two models."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ueapGZwEO8Eo"
      },
      "id": "ueapGZwEO8Eo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "batch_size = 500\n",
        "all_images = os.listdir(data_dir)\n",
        "samples = random.sample(all_images, 500)\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    # Resize the image (assuming 224x224 for this example)\n",
        "    # image = cv2.resize(image, (224, 224))\n",
        "    # Convert the image to a float32 numpy array and normalize it\n",
        "    image = image.astype('float32') / 255.0\n",
        "    image = np.transpose(image, (2, 0, 1))\n",
        "    return image\n",
        "\n",
        "# preprocessed_images = [load_and_preprocess_image(os.path.join(data_dir, img_name)) for img_name in samples]\n",
        "# preprocessed_labels = [get_idx_lbl(img_name)[0] for img_name in samples]\n",
        "# images_tensor = tf.convert_to_tensor(preprocessed_images)\n",
        "# labels_tensor = tf.convert_to_tensor(preprocessed_labels)\n",
        "\n",
        "images = [load_and_preprocess_image(os.path.join(data_dir, img_name)) for img_name in all_images]\n",
        "labels = [get_idx_lbl(img_name)[0] for img_name in all_images]\n",
        "images_tensor = tf.convert_to_tensor(images)\n",
        "labels_tensor = tf.convert_to_tensor(labels)\n",
        "file_names_tensor = tf.convert_to_tensor(all_images)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images_tensor, labels_tensor, file_names_tensor))\n",
        "\n",
        "# # 如果权重不平衡\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# # 计算类别权重\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(labels_tensor.numpy()), y=labels_tensor.numpy())\n",
        "# # 创建一个权重映射（class_weights_map），使得每个标签都对应一个权重\n",
        "# class_weights_map = {i: weight for i, weight in enumerate(class_weights)}\n",
        "# # 创建一个加权的数据集\n",
        "# weighted_dataset = tf.data.Dataset.from_tensor_slices((images_tensor, labels_tensor, file_names_tensor, sample_weights))\n",
        "# # 进行加权随机采样\n",
        "# # 这里的steps_per_epoch是您预期的每个epoch中的步骤数，通常等于样本总数除以batch大小\n",
        "# shuffled_dataset = weighted_dataset.shuffle(buffer_size=len(weighted_dataset)).map(lambda x, y, z, _: (x, y, z))\n",
        "\n",
        "\n",
        "buffer_size = 1000\n",
        "shuffled_dataset = dataset.shuffle(buffer_size)\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Define a simple model\n",
        "model1 = Sequential([\n",
        "    Flatten(input_shape=(3, 5, 5)),  # Flatten the input\n",
        "    Dense(128, activation='relu'),       # Dense layer with 128 units\n",
        "    Dense(len(class_labels), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(optimizer='adam',\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "# Define a simple model\n",
        "model2 = Sequential([\n",
        "    Flatten(input_shape=(3, 5, 5)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(len(class_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer='adam',\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "train_size = 1800\n",
        "test_size = 100\n",
        "# 分割数据集为训练集和测试集\n",
        "train_dataset = shuffled_dataset.take(train_size)\n",
        "test_dataset = shuffled_dataset.skip(train_size)\n",
        "\n",
        "# 设置批量大小\n",
        "batch_size = 200\n",
        "\n",
        "# 批量化测试数据\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# 使用模型进行批量预测\n",
        "y_pred1 = []\n",
        "y_pred2 = []\n",
        "y_true = []\n",
        "\n",
        "for img_batch, label_batch, _ in test_dataset:\n",
        "    pred_batch1 = model1.predict(img_batch)\n",
        "    pred_class_batch1 = tf.argmax(pred_batch1, axis=1).numpy()\n",
        "    y_pred1.extend(pred_class_batch1)\n",
        "\n",
        "    pred_batch2 = model2.predict(img_batch)\n",
        "    pred_class_batch2 = tf.argmax(pred_batch2, axis=1).numpy()\n",
        "    y_pred2.extend(pred_class_batch2)\n",
        "    y_true.extend(label_batch.numpy())\n",
        "\n",
        "y_pred1 = np.array(y_pred1)\n",
        "y_pred2 = np.array(y_pred2)\n",
        "y_true = np.array(y_true)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nlEgBOjO8Eo",
        "outputId": "c2605dc7-2133-423c-d1a4-55cfdf24babf"
      },
      "id": "3nlEgBOjO8Eo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Calculate accuracy\n"
      ],
      "metadata": {
        "id": "7qnbBgzVkwOJ"
      },
      "id": "7qnbBgzVkwOJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Accuracy = TP/TOT\n",
        "# Precision = TP/(TP + FP)\n",
        "# Recall = TP/(TP + FN)\n",
        "# F1 = 2 * Precision * Recall / (Precision + Recall)\n",
        "\n",
        "\n",
        "accuracy1 = accuracy_score(y_true, y_pred1)\n",
        "accuracy2 = accuracy_score(y_true, y_pred2)\n",
        "precision1 = precision_score(y_true, y_pred1, average='weighted', zero_division=0)\n",
        "precision2 = precision_score(y_true, y_pred2, average='weighted', zero_division=0)\n",
        "recall1 = recall_score(y_true, y_pred1, average='weighted')\n",
        "recall2 = recall_score(y_true, y_pred2, average='weighted')\n",
        "f1_model1 = f1_score(y_true, y_pred1, average='weighted')\n",
        "f1_model2 = f1_score(y_true, y_pred2, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy1, accuracy2)\n",
        "print(\"Precision of Model 1:\", precision1, \"Precision of Model 2:\", precision2)\n",
        "print(\"Recall of Model 1:\", recall1, \"Recall of Model 2:\", recall2)\n",
        "print(\"F1 Score of Model 1:\", f1_model1, \"F1 Score of Model 2:\", f1_model2)"
      ],
      "metadata": {
        "id": "jkD5Z-tDlRns",
        "outputId": "e0b58510-f50f-47aa-f1ab-b69e9053e5f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jkD5Z-tDlRns",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.245 0.275\n",
            "Precision of Model 1: 0.060024999999999995 Precision of Model 2: 0.07562500000000001\n",
            "Recall of Model 1: 0.245 Recall of Model 2: 0.275\n",
            "F1 Score of Model 1: 0.09642570281124498 F1 Score of Model 2: 0.11862745098039218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Get the correct predictions"
      ],
      "metadata": {
        "id": "yhW0YHo-pJ1X"
      },
      "id": "yhW0YHo-pJ1X"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "correct_predictions_dir = \"model_correct\"\n",
        "os.makedirs(correct_predictions_dir, exist_ok=True)\n",
        "\n",
        "correct_predictions = y_pred1 == y_true\n",
        "\n",
        "correct_files = []\n",
        "index = 0\n",
        "for _, _, file_name_tensor in test_dataset:\n",
        "    for file_name in file_name_tensor:\n",
        "        if correct_predictions[index]:\n",
        "            correct_files.append(file_name.numpy().decode())\n",
        "        index += 1\n",
        "\n",
        "for file_name in correct_files:\n",
        "    source_path = os.path.join(data_dir, file_name)\n",
        "    destination_path = os.path.join(correct_predictions_dir, file_name)\n",
        "    shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "id": "uL5k_IuGpKhq"
      },
      "id": "uL5k_IuGpKhq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}